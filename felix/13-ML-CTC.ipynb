{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial, reduce\n",
    "from collections import deque\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# labelBaseMap = {\n",
    "#     0: \"A\",\n",
    "#     1: \"C\",\n",
    "#     2: \"G\",\n",
    "#     3: \"T\"\n",
    "# }\n",
    "\n",
    "filename = \"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\"\n",
    "\n",
    "RNN_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(dac):\n",
    "    dmin = min(dac)\n",
    "    dmax = max(dac)\n",
    "    return [(d-dmin)/(dmax-dmin) for d in dac]\n",
    "\n",
    "def ohe(v):\n",
    "    tr = [0,0,0,0]\n",
    "    tr[v] = 1\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'r') as h5file:\n",
    "    readIDs = list(h5file['Reads'].keys())\n",
    "    print(f\"{len(readIDs)} reads, keys: {list(h5file['Reads'][readIDs[0]].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processRead(readID, filename, train_validate_split=0.8, min_labels=5):\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X  = []\n",
    "    test_y  = []\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        DAC = list(normalise(h5file['Reads'][readID]['Dacs'][()]))\n",
    "        RTS = deque(list(h5file['Reads'][readID]['Ref_to_signal'][()]))\n",
    "        REF = deque([ohe(x) for x in h5file['Reads'][readID]['Reference'][()]])\n",
    "        \n",
    "    # just get the number, (1-tvs) so that it can be compared to how many items are left    \n",
    "    train_validate_split = round(len(REF)*(1-train_validate_split))\n",
    "    \n",
    "    # -5 so that the first WHILE iteration afterwards cancels it out\n",
    "    curdacs  = deque( DAC[RTS[0]:RTS[0]+RNN_LEN-5], RNN_LEN ) # deque keeping `RNN_LEN` DACs\n",
    "    curdacts = RTS[0]+RNN_LEN-5 # the current HEAD\n",
    "    labels  = deque([]) # to hold the label for the sequence\n",
    "    labelts = deque([]) # to hold the timestamps for the labels\n",
    "\n",
    "    while RTS[0] < curdacts: # add the first RNN_LEN worth of labels to initialise\n",
    "        labels.append(REF.popleft())\n",
    "        labelts.append(RTS.popleft())    \n",
    "    \n",
    "    \n",
    "    while curdacts+5 < RTS[-1]-RNN_LEN:\n",
    "        curdacs.extend(DAC[curdacts:curdacts+5])\n",
    "        curdacts += 5\n",
    "        \n",
    "        # add labels if new ones appeared\n",
    "        while RTS[0] < curdacts:\n",
    "            labels.append(REF.popleft())\n",
    "            labelts.append(RTS.popleft())    \n",
    "        \n",
    "        # pop from labels if we passed them\n",
    "        # sometimes the strand gets stuck and the deques drop to 0\n",
    "        while len(labelts) > 0 and labelts[0] < curdacts - RNN_LEN:\n",
    "            labels.popleft()\n",
    "            labelts.popleft()\n",
    "\n",
    "        # only add if more than 5 labels\n",
    "        if len(labels) > min_labels:\n",
    "            # add to train if more than tvs remain\n",
    "            if len(RTS) > train_validate_split:\n",
    "                train_X.append(list(curdacs))\n",
    "                train_y.append(list(labels))\n",
    "            else:\n",
    "                test_X.append(list(curdacs))\n",
    "                test_y.append(list(labels))\n",
    "                \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "pp = partial(processRead, filename=filename)\n",
    "tr_X, tr_y, te_X, te_y = pp(readIDs[1])\n",
    "\n",
    "len(te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(16)\n",
    "results_prim = pool.map(partial(processRead, filename=filename), readIDs[:40])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X  = []\n",
    "test_y  = []\n",
    "for thread in results_prim:\n",
    "    train_X.extend(thread[0])\n",
    "    train_y.extend(thread[1])\n",
    "    test_X.extend(thread[2])\n",
    "    test_y.extend(thread[3])\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "test_X  = np.array(test_X)\n",
    "test_y  = np.array(test_y)\n",
    "\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten, Conv1D, LSTM, Softmax\n",
    "from tensorflow.nn import ctc_loss\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\", \n",
    "          input_shape=X[0].shape))\n",
    "model.add(Conv1D(32, 10,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\"))\n",
    "model.add(Conv1D(32, 5,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(64))\n",
    "model.compile(optimizer=\"adam\", loss=ctc_loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=X, y=y, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
