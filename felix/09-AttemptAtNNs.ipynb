{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten, Conv1D, LSTM, Embedding, SpatialDropout1D, Dropout, SimpleRNN\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from taiyaki encoding\n",
    "labelBaseMap = {\n",
    "    0: \"A\",\n",
    "    1: \"C\",\n",
    "    2: \"G\",\n",
    "    3: \"T\"\n",
    "}\n",
    "\n",
    "def get_reads_dict(filename):\n",
    "    file = h5py.File(filename, \"r\")\n",
    "    file = file['Reads']\n",
    "    reads = []\n",
    "    for r in file.keys():\n",
    "        elem = {}\n",
    "        elem['UUID'] = r\n",
    "        firstMap = file[r]['Ref_to_signal'][0] # get the first taiyaki map point\n",
    "        lastMap = file[r]['Ref_to_signal'][-1] # get the last taiyaki map point\n",
    "        elem['Dacs'] = normalise_list(file[r]['Dacs'][firstMap:lastMap]) # Cut the signal to the last and first point\n",
    "        elem['Ref_to_signal'] = file[r]['Ref_to_signal']-firstMap # now the first map point is 0\n",
    "        elem['Reference'] = file[r]['Reference']\n",
    "        reads.append(elem)\n",
    "    return reads\n",
    "\n",
    "def make_dna_list(read):\n",
    "    dnalist = []\n",
    "    pos = 0\n",
    "    signalrefs = list(read['Ref_to_signal'])\n",
    "    dnaref = list(read['Reference'])\n",
    "    for idx in range(len(signalrefs)):\n",
    "        if idx == 0:\n",
    "            dnalist.extend([-1]*signalrefs[idx])\n",
    "        else:\n",
    "            for i in range(signalrefs[idx]-signalrefs[idx-1]):\n",
    "                dnalist.append(dnaref[idx-1])\n",
    "    return dnalist\n",
    "\n",
    "def normalise_list(lst):\n",
    "    mmin = min(lst)\n",
    "    mmax = max(lst)\n",
    "    lst = (lst - mmin)/(mmax - mmin)\n",
    "    return lst\n",
    "\n",
    "def intbase_to_vec(base):\n",
    "    vec = [0]*4\n",
    "    vec[base]=1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = get_reads_dict(\"../taiyakiOutputs/output_createfasta.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_per = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for r in reads[:2]:\n",
    "    dnalist = list(map(intbase_to_vec, make_dna_list(r)))\n",
    "    # dnalist = make_dna_list(r)\n",
    "    sig = r['Dacs']\n",
    "    \n",
    "    for i in range(reads_per, len(dnalist)-reads_per):\n",
    "        if random.random() > 0.7:\n",
    "            train_X.append(sig[i-reads_per:i+reads_per])\n",
    "            train_y.append(dnalist[i])\n",
    "        else:\n",
    "            test_X.append(sig[i-reads_per:i+reads_per])\n",
    "            test_y.append(dnalist[i])\n",
    "\n",
    "train_X2 = np.expand_dims(train_X, axis=2)\n",
    "test_X2 = np.expand_dims(test_X, axis=2)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(12, input_dim=reads_per*2, activation='relu'))\n",
    "model_nn.add(Dense(8, activation='relu'))\n",
    "model_nn.add(Dense(4, activation='softmax'))\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_nn.fit([train_X], [train_y], epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.evaluate([test_X], [test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D (kernel_size = (20), filters = 20, input_shape=(reads_per*2,1), activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(pool_size = (20), strides=(10)))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(12, activation='relu'))\n",
    "model_cnn.add(Dense(8, activation='relu'))\n",
    "model_cnn.add(Dense(4, activation='softmax'))\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit([train_X2], [train_y], epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(40, input_shape=(reads_per*2, 1), dropout = 0.1))\n",
    "model_rnn.add(Dense(20, activation='relu'))\n",
    "model_rnn.add(Dense(8, activation='relu'))\n",
    "model_rnn.add(Dense(4, activation='softmax'))\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model_rnn.fit(train_X2, [train_y], epochs=2, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublelayer = False\n",
    "# LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(40, input_shape=(reads_per*2, 1), dropout = 0.1, return_sequences=doublelayer))\n",
    "if doublelayer:\n",
    "    model_lstm.add(LSTM(40, input_shape=(reads_per*2, 1), dropout = 0.1))\n",
    "model_lstm.add(Dense(20, activation='relu'))\n",
    "model_lstm.add(Dense(8, activation='relu'))\n",
    "model_lstm.add(Dense(4, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(train_X2, [train_y], epochs=2, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
