{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial, reduce\n",
    "from collections import deque\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# labelBaseMap = {\n",
    "#     0: \"A\",\n",
    "#     1: \"C\",\n",
    "#     2: \"G\",\n",
    "#     3: \"T\"\n",
    "# }\n",
    "\n",
    "possible_filenames = [\"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\",\n",
    "                      \"/Users/felix/MsC/DNA/mapped_umi16to9.hdf5\"]\n",
    "\n",
    "RNN_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(dac):\n",
    "    dmin = min(dac)\n",
    "    dmax = max(dac)\n",
    "    return [(d-dmin)/(dmax-dmin) for d in dac]\n",
    "\n",
    "def ohe(v):\n",
    "    tr = [0,0,0,0]\n",
    "    tr[v] = 1\n",
    "    return tr\n",
    "\n",
    "def change_shape(dac):\n",
    "    return [[x] for x in dac]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in possible_filenames:\n",
    "    if not os.path.isfile(filename):\n",
    "        pass\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        readIDs = list(h5file['Reads'].keys())\n",
    "        print(f\"{len(readIDs)} reads, keys: {list(h5file['Reads'][readIDs[0]].keys())}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRead(readID, filename, train_validate_split=0.8, min_labels=5, one_hot_encode=False):\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X  = []\n",
    "    test_y  = []\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        DAC = list(normalise(h5file['Reads'][readID]['Dacs'][()]))\n",
    "        RTS = deque(list(h5file['Reads'][readID]['Ref_to_signal'][()]))\n",
    "        if ohe:\n",
    "            REF = deque([ohe(x) for x in h5file['Reads'][readID]['Reference'][()]])\n",
    "        else:\n",
    "            REF = deque(h5file['Reads'][readID]['Reference'][()])\n",
    "        \n",
    "    # just get the number, (1-tvs) so that it can be compared to how many items are left    \n",
    "    train_validate_split = round(len(REF)*(1-train_validate_split))\n",
    "    \n",
    "    # -5 so that the first WHILE iteration afterwards cancels it out\n",
    "    curdacs  = deque( [[x] for x in DAC[RTS[0]:RTS[0]+RNN_LEN-5]], RNN_LEN ) # deque keeping `RNN_LEN` DACs\n",
    "    curdacts = RTS[0]+RNN_LEN-5 # the current HEAD\n",
    "    labels  = deque([]) # to hold the label for the sequence\n",
    "    labelts = deque([]) # to hold the timestamps for the labels\n",
    "\n",
    "    while RTS[0] < curdacts: # add the first RNN_LEN worth of labels to initialise\n",
    "        labels.append(REF.popleft())\n",
    "        labelts.append(RTS.popleft())    \n",
    "    \n",
    "    \n",
    "    while curdacts+5 < RTS[-1]-RNN_LEN:\n",
    "        curdacs.extend([[x] for x in DAC[curdacts:curdacts+5]])\n",
    "        curdacts += 5\n",
    "        \n",
    "        # add labels if new ones appeared\n",
    "        while RTS[0] < curdacts:\n",
    "            labels.append(REF.popleft())\n",
    "            labelts.append(RTS.popleft())    \n",
    "        \n",
    "        # pop from labels if we passed them\n",
    "        # sometimes the strand gets stuck and the deques drop to 0\n",
    "        while len(labelts) > 0 and labelts[0] < curdacts - RNN_LEN:\n",
    "            labels.popleft()\n",
    "            labelts.popleft()\n",
    "\n",
    "        # only add if more than 5 labels\n",
    "        if len(labels) > min_labels:\n",
    "            # add to train if more than tvs remain\n",
    "            if len(RTS) > train_validate_split:\n",
    "                train_X.append(list(curdacs))\n",
    "                train_y.append(list(labels))\n",
    "            else:\n",
    "                test_X.append(list(curdacs))\n",
    "                test_y.append(list(labels))\n",
    "                \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "# pp = partial(processRead, filename=filename, one_hot_encode=True)\n",
    "# tr_X, tr_y, te_X, te_y = pp(readIDs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(16)\n",
    "results_prim = pool.map(partial(processRead, filename=filename, one_hot_encode=True), readIDs[:2])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Pool done\")\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X  = []\n",
    "test_y  = []\n",
    "for thread in results_prim:\n",
    "    train_X.extend(thread[0])\n",
    "    train_y.extend(thread[1])\n",
    "    test_X.extend(thread[2])\n",
    "    test_y.extend(thread[3])\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "test_X  = np.array(test_X)\n",
    "test_y  = np.array(test_y)\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten, Conv1D, Softmax, Dropout, LSTM\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "device_hasgpu = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "if device_hasgpu:\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_custom(prediction_batch, label_batch):\n",
    "    input_length = kb.map_fn(lambda x: np.array([20]), prediction_batch)\n",
    "    label_length = kb.map_fn(lambda x: np.array([x.shape[1]]), label_batch)\n",
    "    return ctc_batch_cost(label_batch, prediction_batch, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, 3,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\", \n",
    "          input_shape=train_X[0].shape))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 10,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 5,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(32,return_sequences=True))\n",
    "# model.add(CuDNNLSTM(32))\n",
    "\n",
    "# model.add(Dense(64, activation=\"relu\"))\n",
    "# model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=ctc_custom, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "train_yy = np.array([y[-1] for y in train_y])\n",
    "test_yy = np.array([y[-1] for y in test_y])\n",
    "\n",
    "print(train_yy.shape)\n",
    "print(test_yy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_X, y=train_yy, epochs=2, batch_size=10, validation_data=(test_X, test_yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[20] for x in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([[1,2,3,4],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.map_fn(lambda x: np.array([20]), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.map_fn(lambda x: np.array([x.shape[0]]), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
