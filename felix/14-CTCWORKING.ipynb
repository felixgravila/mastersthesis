{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Lambda\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, LSTM\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import editdistance\n",
    "\n",
    "labelBaseMap = {\n",
    "    0: \"A\",\n",
    "    1: \"C\",\n",
    "    2: \"G\",\n",
    "    3: \"T\"\n",
    "}\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X = np.load(\"train_X.npy\")\n",
    "train_y = np.load(\"train_y.npy\", allow_pickle=True)\n",
    "test_X  = np.load(\"test_X.npy\")\n",
    "test_y  = np.load(\"test_y.npy\", allow_pickle=True)\n",
    "\n",
    "train_X_lens = np.array([[95] for x in train_X], dtype=\"float32\")\n",
    "train_y_lens = np.array([[len(x)] for x in train_y], dtype=\"float32\")\n",
    "\n",
    "maxlen = max([len(r) for r in train_y])\n",
    "train_y_padded = np.array([r + [5]*(maxlen-len(r)) for r in train_y], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'the_input': train_X,\n",
    "          'the_labels': train_y_padded,\n",
    "          'input_length': train_X_lens,\n",
    "          'label_length': train_y_lens\n",
    "          }\n",
    "outputs = {'ctc': np.zeros([len(train_X)])}\n",
    "training_data = (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    y_pred = y_pred[:, 5:, :]\n",
    "    return kb.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def decode_batch(modelfunc, input_data):\n",
    "    pred = modelfunc(input_data)[0]\n",
    "    cur = [[np.argmax(ts) for ts in p] for p in pred]\n",
    "    nodup = [\"\".join(list(map(lambda x: labelBaseMap[x], filter(lambda x: x!=4, reduce(lambda acc, x: acc if acc[-1] == x else acc + [x], c[5:], [4]))))) for c in cur]\n",
    "    return nodup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(name=\"the_input\", shape=(200,1), dtype=\"float32\")\n",
    "inner = Conv1D(32, 3,\n",
    "          padding=\"same\",\n",
    "          activation=\"relu\",\n",
    "          name=\"conv1d_1\")(input_data)\n",
    "inner = MaxPooling1D(pool_size=2, name=\"maxpool_1\")(inner)\n",
    "inner = Conv1D(32, 5,\n",
    "          padding=\"same\",\n",
    "          activation=\"relu\",\n",
    "          name=\"conv1d_2\")(inner)\n",
    "inner = Conv1D(32, 3,\n",
    "          padding=\"same\",\n",
    "          activation=\"relu\",\n",
    "          name=\"conv1d_3\")(inner)\n",
    "\n",
    "lstm_1a = LSTM(32, return_sequences=True, name=\"lstm_1a\")(inner)\n",
    "lstm_1b = LSTM(32, return_sequences=True, go_backwards=True, name=\"lstm_1b\")(inner)\n",
    "lstm_1_merged = Add()([lstm_1a, lstm_1b])\n",
    "\n",
    "lstm_2a = LSTM(32, return_sequences=True, name=\"lstm_2a\")(lstm_1_merged)\n",
    "lstm_2b = LSTM(32, return_sequences=True, go_backwards=True, name=\"lstm_2b\")(lstm_1_merged)\n",
    "lstm_2_merged = Add()([lstm_2a, lstm_2b])\n",
    "\n",
    "inner = Dense(5, name=\"dense_1\")(lstm_2_merged)\n",
    "\n",
    "y_pred = Activation(\"softmax\", name=\"softmax\")(inner)\n",
    "\n",
    "# Model(inputs=input_data, outputs=y_pred).summary()\n",
    "\n",
    "labels = Input(name='the_labels', shape=(maxlen), dtype='float32')\n",
    "input_length = Input(name='input_length', shape=(1), dtype='int64')\n",
    "label_length = Input(name='label_length', shape=(1), dtype='int64')\n",
    "\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out, name=\"my_model\")\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValCB(Callback):\n",
    "    def __init__(self, testfunc, test_X, test_y):\n",
    "        self.testfunc = testfunc\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print('Training: batch {}'.format(batch), end=\"\\r\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"\")\n",
    "#         self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n",
    "        predicted = decode_batch(self.testfunc, self.test_X)\n",
    "        mtest_y = [\"\".join(list(map(lambda x: labelBaseMap[x], ty))) for ty in self.test_y]\n",
    "        tot_edit_dis = 0\n",
    "        for (p,l) in zip(predicted, mtest_y):\n",
    "            tot_edit_dis += editdistance.eval(p,l)\n",
    "            \n",
    "        print(f\"Average edit distance is: {tot_edit_dis/len(mtest_y)}\\n\")\n",
    "\n",
    "testfunc = tf.keras.backend.function([input_data], [y_pred])\n",
    "validation_cb = ValCB(testfunc, test_X[:100], test_y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3])\n",
    "plt.show()\n",
    "model.fit(x=inputs, y=outputs, epochs=5, callbacks=[validation_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
