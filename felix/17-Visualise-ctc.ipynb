{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial, reduce\n",
    "from collections import deque\n",
    "from IPython.core.debugger import set_trace\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "labelBaseMap = {\n",
    "    0: \"A\",\n",
    "    1: \"C\",\n",
    "    2: \"G\",\n",
    "    3: \"T\"\n",
    "}\n",
    "\n",
    "possible_filenames = [\"/mnt/sdb/taiyaki_mapped/mapped_umi16to9.hdf5\",\n",
    "                      \"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\",\n",
    "                      \"/Users/felix/MsC/DNA/mapped_umi16to9.hdf5\"]\n",
    "\n",
    "for filename in possible_filenames:\n",
    "    if os.path.isfile(filename):\n",
    "        the_filename = filename\n",
    "        print(f\"Using {filename}\")\n",
    "        break\n",
    "else:\n",
    "    the_filename = \"\"\n",
    "    print(\"Error, no filename valid!\")\n",
    "\n",
    "RNN_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PrepData(Sequence):\n",
    "    \n",
    "    def __init__(self, filename, train_validate_split=0.8, min_labels=5):\n",
    "        self.filename = filename\n",
    "        self.train_validate_split=train_validate_split\n",
    "        self.min_labels=min_labels\n",
    "        self.pos = 0\n",
    "        self.test_gen_data = ([],[])\n",
    "        self.max_label_len = 50\n",
    "        with h5py.File(filename, 'r') as h5file:\n",
    "            self.readIDs = list(h5file['Reads'].keys())\n",
    "            \n",
    "    def get_len(self):\n",
    "        return len(self.readIDs)\n",
    "    \n",
    "    def get_max_label_len(self):\n",
    "        return self.max_label_len\n",
    "        \n",
    "    def normalise(self, dac):\n",
    "        dmin = min(dac)\n",
    "        dmax = max(dac)\n",
    "        return [(d-dmin)/(dmax-dmin) for d in dac]\n",
    "    \n",
    "    def processRead(self, readID):\n",
    "        train_X = []\n",
    "        train_y = []\n",
    "        test_X  = []\n",
    "        test_y  = []\n",
    "        with h5py.File(self.filename, 'r') as h5file:\n",
    "            DAC = list(self.normalise(h5file['Reads'][readID]['Dacs'][()]))\n",
    "            RTS = deque(list(h5file['Reads'][readID]['Ref_to_signal'][()]))\n",
    "            REF = deque(h5file['Reads'][readID]['Reference'][()])\n",
    "            \n",
    "        train_validate_split = round(len(REF)*(1-self.train_validate_split))\n",
    "        curdacs  = deque( [[x] for x in DAC[RTS[0]:RTS[0]+RNN_LEN-5]], RNN_LEN )\n",
    "        curdacts = RTS[0]+RNN_LEN-5\n",
    "        labels  = deque([])\n",
    "        labelts = deque([])\n",
    "\n",
    "        while RTS[0] < curdacts:\n",
    "            labels.append(REF.popleft())\n",
    "            labelts.append(RTS.popleft())\n",
    "\n",
    "\n",
    "        while curdacts+5 < RTS[-1]-RNN_LEN:\n",
    "            curdacs.extend([[x] for x in DAC[curdacts:curdacts+5]])\n",
    "            curdacts += 5\n",
    "            \n",
    "            while RTS[0] < curdacts:\n",
    "                labels.append(REF.popleft())\n",
    "                labelts.append(RTS.popleft())\n",
    "                \n",
    "            while len(labelts) > 0 and labelts[0] < curdacts - RNN_LEN:\n",
    "                labels.popleft()\n",
    "                labelts.popleft()\n",
    "\n",
    "            if len(labels) > self.min_labels:\n",
    "                if len(RTS) > train_validate_split:\n",
    "                    train_X.append(list(curdacs))\n",
    "                    train_y.append(list(labels))\n",
    "                else:\n",
    "                    test_X.append(list(curdacs))\n",
    "                    test_y.append(list(labels))\n",
    "\n",
    "        return train_X, train_y, test_X, test_y\n",
    "    \n",
    "    \n",
    "    def train_gen(self):\n",
    "        while self.pos < len(self.readIDs):\n",
    "            print(f\"Processing {self.pos}\")\n",
    "            train_X, train_y, test_X, test_y = self.processRead(self.readIDs[self.pos])\n",
    "            self.pos += 1\n",
    "            \n",
    "            train_X = np.array(train_X)\n",
    "            train_y = np.array(train_y)\n",
    "            test_X  = np.array(test_X)\n",
    "            test_y  = np.array(test_y)\n",
    "            self.test_gen_data = (test_X, test_y)\n",
    "            \n",
    "            train_X_lens = np.array([[95] for x in train_X], dtype=\"float32\")\n",
    "            train_y_lens = np.array([[len(x)] for x in train_y], dtype=\"float32\")\n",
    "#             maxlen = max([len(r) for r in train_y])\n",
    "            train_y_padded = np.array([r + [5]*(self.get_max_label_len()-len(r)) for r in train_y], dtype='float32')\n",
    "            X = {'the_input': train_X,\n",
    "                      'the_labels': train_y_padded,\n",
    "                      'input_length': train_X_lens,\n",
    "                      'label_length': train_y_lens\n",
    "                      }\n",
    "            y = {'ctc': np.zeros([len(train_X)])}\n",
    "            yield (X, y)\n",
    "        \n",
    "    def test_gen(self):\n",
    "        while True:\n",
    "            tgd, self.test_gen_data = self.test_gen_data, ([],[])\n",
    "            yield tgd\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.readIDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return next(self.train_gen())\n",
    "    \n",
    "prepData = PrepData(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Lambda\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, LSTM, GRU\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import editdistance\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_res_block(upper, block):\n",
    "    res = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  name=f\"res{block}-r\")(upper)\n",
    "    upper = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c1\")(upper)\n",
    "    upper = Conv1D(256, 3,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c2\")(upper)\n",
    "    upper = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c3\")(upper)\n",
    "    added = Add(name=f\"res{block}-add\")([res, upper])\n",
    "    return Activation('relu', name=f\"res{block}-relu\")(added)\n",
    "\n",
    "def make_bdlstm(upper, block):\n",
    "    lstm_1a = LSTM(200, return_sequences=True, name=f\"blstm{block}-fwd\")(upper)\n",
    "    lstm_1b = LSTM(200, return_sequences=True, go_backwards=True, name=f\"blstm{block}-rev\")(upper)\n",
    "    return Add(name=f\"blstm{block}-add\")([lstm_1a, lstm_1b])\n",
    "\n",
    "\n",
    "input_data = Input(name=\"the_input\", shape=(300,1), dtype=\"float32\")\n",
    "\n",
    "inner = make_res_block(input_data, 1)\n",
    "inner = make_res_block(inner, 2)\n",
    "inner = make_res_block(inner, 3)\n",
    "inner = make_res_block(inner, 4)\n",
    "inner = make_res_block(inner, 5)\n",
    "inner = make_bdlstm(inner, 1)\n",
    "inner = make_bdlstm(inner, 2)\n",
    "inner = make_bdlstm(inner, 3)\n",
    "\n",
    "inner = Dense(64, name=\"dense\", activation=\"relu\")(inner)\n",
    "inner = Dense(5, name=\"dense_output\")(inner)\n",
    "\n",
    "y_pred = Activation(\"softmax\", name=\"softmax\")(inner)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=y_pred)\n",
    "model.load_weights(\"weights/2020-01-07_13:49:58_e14_dis5427.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(prepData.train_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 200\n",
    "ipt = a[0]['the_input'][idx:idx+1]\n",
    "lbs = a[0]['the_labels'][idx:idx+1]\n",
    "print(a[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(ipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"weights\"):\n",
    "    model.load_weights(f\"weights/{file}\")\n",
    "    prediction = model.predict(ipt)\n",
    "    plt.figure(figsize=(30,10))\n",
    "    for pred, raw, label in zip(prediction, ipt, lbs):\n",
    "        transposed = list(map(list, zip(*pred)))\n",
    "        for i in range(len(transposed)):\n",
    "            plt.plot(transposed[i], label=i)\n",
    "        plt.plot(raw, \"k\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"images/{file}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]['label_length'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
