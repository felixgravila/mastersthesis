{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial, reduce\n",
    "from collections import deque\n",
    "from IPython.core.debugger import set_trace\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "labelBaseMap = {\n",
    "    0: \"A\",\n",
    "    1: \"C\",\n",
    "    2: \"G\",\n",
    "    3: \"T\",\n",
    "    4: \"-\"\n",
    "}\n",
    "\n",
    "possible_filenames = [\"/mnt/sdb/taiyaki_mapped/mapped_umi16to9.hdf5\",\n",
    "                      \"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\",\n",
    "                      \"/Users/felix/MsC/DNA/mapped_umi16to9.hdf5\"]\n",
    "\n",
    "for filename in possible_filenames:\n",
    "    if os.path.isfile(filename):\n",
    "        the_filename = filename\n",
    "        print(f\"Using {filename}\")\n",
    "        break\n",
    "else:\n",
    "    the_filename = \"\"\n",
    "    print(\"Error, no filename valid!\")\n",
    "\n",
    "RNN_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PrepData(Sequence):\n",
    "    \n",
    "    def __init__(self, filename, train_validate_split=0.8, min_labels=5):\n",
    "        self.filename = filename\n",
    "        self.train_validate_split=train_validate_split\n",
    "        self.min_labels=min_labels\n",
    "        self.pos = 0\n",
    "        self.test_gen_data = ([],[])\n",
    "        self.last_train_gen_data = ({},{})\n",
    "        self.max_label_len = 50\n",
    "        with h5py.File(filename, 'r') as h5file:\n",
    "            self.readIDs = list(h5file['Reads'].keys())\n",
    "            \n",
    "    def get_len(self):\n",
    "        return len(self.readIDs)\n",
    "    \n",
    "    def get_max_label_len(self):\n",
    "        return self.max_label_len\n",
    "        \n",
    "    def normalise(self, dac):\n",
    "        dmin = min(dac)\n",
    "        dmax = max(dac)\n",
    "        return [(d-dmin)/(dmax-dmin) for d in dac]\n",
    "    \n",
    "    def processRead(self, readID):\n",
    "        train_X = []\n",
    "        train_y = []\n",
    "        test_X  = []\n",
    "        test_y  = []\n",
    "        with h5py.File(self.filename, 'r') as h5file:\n",
    "            DAC = list(self.normalise(h5file['Reads'][readID]['Dacs'][()]))\n",
    "            RTS = deque(list(h5file['Reads'][readID]['Ref_to_signal'][()]))\n",
    "            REF = deque(h5file['Reads'][readID]['Reference'][()])\n",
    "            \n",
    "        train_validate_split = round(len(REF)*(1-self.train_validate_split))\n",
    "        curdacs  = deque( [[x] for x in DAC[RTS[0]:RTS[0]+RNN_LEN-5]], RNN_LEN )\n",
    "        curdacts = RTS[0]+RNN_LEN-5\n",
    "        labels  = deque([])\n",
    "        labelts = deque([])\n",
    "\n",
    "        while RTS[0] < curdacts:\n",
    "            labels.append(REF.popleft())\n",
    "            labelts.append(RTS.popleft())\n",
    "\n",
    "\n",
    "        while curdacts+5 < RTS[-1]-RNN_LEN:\n",
    "            curdacs.extend([[x] for x in DAC[curdacts:curdacts+5]])\n",
    "            curdacts += 5\n",
    "            \n",
    "            while RTS[0] < curdacts:\n",
    "                labels.append(REF.popleft())\n",
    "                labelts.append(RTS.popleft())\n",
    "                \n",
    "            while len(labelts) > 0 and labelts[0] < curdacts - RNN_LEN:\n",
    "                labels.popleft()\n",
    "                labelts.popleft()\n",
    "\n",
    "            if len(labels) > self.min_labels:\n",
    "                if len(RTS) > train_validate_split:\n",
    "                    train_X.append(list(curdacs))\n",
    "                    train_y.append(list(labels))\n",
    "                else:\n",
    "                    test_X.append(list(curdacs))\n",
    "                    test_y.append(list(labels))\n",
    "\n",
    "        return train_X, train_y, test_X, test_y\n",
    "    \n",
    "    \n",
    "    def train_gen(self, full=True):\n",
    "        while self.pos < len(self.readIDs):\n",
    "            print(f\"Processing {self.pos}\")\n",
    "            train_X, train_y, test_X, test_y = self.processRead(self.readIDs[self.pos])\n",
    "            self.pos += 1\n",
    "            \n",
    "            train_X = np.array(train_X) if full else np.array(train_X[:100])\n",
    "            train_y = np.array(train_y) if full else np.array(train_y[:100])\n",
    "            test_X  = np.array(test_X) if full else np.array(test_X[:100])\n",
    "            test_y  = np.array(test_y) if full else np.array(test_y[:100])\n",
    "            self.test_gen_data = (test_X, test_y)\n",
    "            \n",
    "            train_X_lens = np.array([[95] for x in train_X], dtype=\"float32\")\n",
    "            train_y_lens = np.array([[len(x)] for x in train_y], dtype=\"float32\")\n",
    "#             maxlen = max([len(r) for r in train_y])\n",
    "            train_y_padded = np.array([r + [5]*(self.get_max_label_len()-len(r)) for r in train_y], dtype='float32')\n",
    "            X = {'the_input': train_X,\n",
    "                      'the_labels': train_y_padded,\n",
    "                      'input_length': train_X_lens,\n",
    "                      'label_length': train_y_lens,\n",
    "                      'unpadded_labels' : train_y\n",
    "                      }\n",
    "            y = {'ctc': np.zeros([len(train_X)])}\n",
    "            self.last_train_gen_data = (X, y)\n",
    "            yield (X, y)\n",
    "        \n",
    "    def test_gen(self):\n",
    "        while True:\n",
    "            tgd, self.test_gen_data = self.test_gen_data, ([],[])\n",
    "            yield tgd\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.readIDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return next(self.train_gen())\n",
    "    \n",
    "prepData = PrepData(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Lambda\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, LSTM, GRU\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import editdistance\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    y_pred = y_pred[:, 5:, :]\n",
    "    return kb.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def decode_batch(modelfunc, input_data):\n",
    "    pred = modelfunc(input_data)[0]\n",
    "    cur = [[np.argmax(ts) for ts in p] for p in pred]\n",
    "    nodup = [\"\".join(list(map(lambda x: labelBaseMap[x], filter(lambda x: x!=4, reduce(lambda acc, x: acc if acc[-1] == x else acc + [x], c[5:], [4]))))) for c in cur]\n",
    "    return nodup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_res_block(upper, block):\n",
    "    res = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  name=f\"res{block}-r\")(upper)\n",
    "    upper = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c1\")(upper)\n",
    "    upper = Conv1D(256, 3,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c2\")(upper)\n",
    "    upper = Conv1D(256, 1,\n",
    "                  padding=\"same\",\n",
    "                  use_bias=\"false\",\n",
    "                  name=f\"res{block}-c3\")(upper)\n",
    "    added = Add(name=f\"res{block}-add\")([res, upper])\n",
    "    return Activation('relu', name=f\"res{block}-relu\")(added)\n",
    "\n",
    "def make_bdlstm(upper, block):\n",
    "    lstm_1a = LSTM(200, return_sequences=True, name=f\"blstm{block}-fwd\")(upper)\n",
    "    lstm_1b = LSTM(200, return_sequences=True, go_backwards=True, name=f\"blstm{block}-rev\")(upper)\n",
    "    return Add(name=f\"blstm{block}-add\")([lstm_1a, lstm_1b])\n",
    "\n",
    "\n",
    "input_data = Input(name=\"the_input\", shape=(300,1), dtype=\"float32\")\n",
    "\n",
    "inner = make_res_block(input_data, 1)\n",
    "inner = make_res_block(inner, 2)\n",
    "inner = make_res_block(inner, 3)\n",
    "inner = make_res_block(inner, 4)\n",
    "inner = make_res_block(inner, 5)\n",
    "inner = make_bdlstm(inner, 1)\n",
    "inner = make_bdlstm(inner, 2)\n",
    "inner = make_bdlstm(inner, 3)\n",
    "\n",
    "inner = Dense(64, name=\"dense\", activation=\"relu\")(inner)\n",
    "inner = Dense(5, name=\"dense_output\")(inner)\n",
    "\n",
    "y_pred = Activation(\"softmax\", name=\"softmax\")(inner)\n",
    "\n",
    "labels = Input(name='the_labels', shape=(prepData.get_max_label_len()), dtype='float32')\n",
    "input_length = Input(name='input_length', shape=(1), dtype='int64')\n",
    "label_length = Input(name='label_length', shape=(1), dtype='int64')\n",
    "\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out, name=\"chiron\")\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCB(Callback):\n",
    "    def __init__(self, model_output_dir, image_output_dir, test_func, prepper):\n",
    "        self.model_output_dir=model_output_dir\n",
    "        self.image_output_dir=image_output_dir\n",
    "        self.test_func = test_func\n",
    "        self.prepper = prepper\n",
    "        self.best_dist = None\n",
    "        self.Xforimg = None\n",
    "        self.testvalid = [[],[]]\n",
    "        \n",
    "    def save_anim_pic(self, epoch):\n",
    "        fig, ax = plt.subplots( nrows=1, ncols=1, figsize=(30,10))\n",
    "        ax.set_ylim(top=1)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        prediction = self.test_func(self.Xforimg)[0][0]\n",
    "        transposed = list(map(list, zip(*prediction)))\n",
    "        for i in range(len(transposed)):\n",
    "            ax.plot(transposed[i], label=labelBaseMap[i])\n",
    "        ax.plot(self.Xforimg[0], \"k\", label=\"raw\")\n",
    "        ax.legend()\n",
    "        fig.savefig(os.path.join(self.image_output_dir, f'{epoch:05d}.png'))\n",
    "        plt.close(fig)\n",
    "        \n",
    "    def calculate_loss(self, X, y, testbatchsize=1000):\n",
    "        editdis = 0\n",
    "        for b in range(0, len(X), testbatchsize):\n",
    "            predicted = decode_batch(self.test_func, X[b:b+testbatchsize])\n",
    "            mtest_y = [\"\".join(list(map(lambda x: labelBaseMap[x], ty))) for ty in y[b:b+testbatchsize]]\n",
    "            for (p,l) in zip(predicted, mtest_y):\n",
    "                editdis += editdistance.eval(p,l)\n",
    "        return editdis/len(y)\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        test_X, test_y = next(self.prepper.test_gen())\n",
    "        train_X, train_y = self.prepper.last_train_gen_data[0]['the_input'], self.prepper.last_train_gen_data[0]['unpadded_labels']\n",
    "#         if self.Xforimg is None:\n",
    "#             self.Xforimg = test_X[0:1]\n",
    "#         self.save_anim_pic(epoch)\n",
    "\n",
    "        testloss = self.calculate_loss(train_X, train_y)\n",
    "        print(f\"\\nAverage test edit distance is: {testloss}\")\n",
    "        valloss = self.calculate_loss(test_X, test_y)\n",
    "        print(f\"\\nAverage validation edit distance is: {valloss}\")\n",
    "        self.testvalid[0].append(testloss)\n",
    "        self.testvalid[1].append(valloss)\n",
    "        np.save(\"testval\", np.array(np.array(self.testvalid)))\n",
    "        \n",
    "        if self.best_dist is None or valloss < self.best_dist:\n",
    "            self.best_dist = valloss\n",
    "            self.model.save_weights(os.path.join(self.model_output_dir, f'{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}_e{epoch:05d}_dis{round(valloss*100)}.h5'))\n",
    "        \n",
    "save_cb = SaveCB(\"models\", \"images\", tf.keras.backend.function([input_data], [y_pred]), prepData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(prepData.get_len()):\n",
    "    print(f\"Epoch {idx}/{prepData.get_len()}\")\n",
    "    try:\n",
    "        a = next(prepData.train_gen())\n",
    "        model.fit(a[0], a[1], initial_epoch=idx, epochs=idx+1, callbacks=[save_cb])\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}, continuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
