{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial, reduce\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten, Conv1D, CuDNNLSTM, Softmax\n",
    "from tensorflow.nn import ctc_loss\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "# labelBaseMap = {\n",
    "#     0: \"A\",\n",
    "#     1: \"C\",\n",
    "#     2: \"G\",\n",
    "#     3: \"T\"\n",
    "# }\n",
    "\n",
    "filename = \"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\"\n",
    "\n",
    "RNN_LEN = 200\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'r') as h5file:\n",
    "    readIDs = list(h5file['Reads'].keys())\n",
    "    print(f\"{len(readIDs)} reads, keys: {list(h5file['Reads'][readIDs[0]].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRead(readID, filename):\n",
    "    data = []\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        DAC = list(h5file['Reads'][readID]['Dacs'][()])\n",
    "        RTS = list(h5file['Reads'][readID]['Ref_to_signal'][()])\n",
    "        REF = list(h5file['Reads'][readID]['Reference'][()])\n",
    "    for rtsidx in range(len(RTS)-1):\n",
    "        # Add to dataset in increments of 5 until too close to the next rtsidx\n",
    "        # Or not enough Dacs left\n",
    "        i = RTS[rtsidx]\n",
    "\n",
    "        #make the labels iteratively\n",
    "        labels = []\n",
    "        l = rtsidx\n",
    "        while RTS[l] < i + RNN_LEN and l < len(REF):\n",
    "            labels.append(REF[l])\n",
    "            l += 1\n",
    "\n",
    "        while i < (RTS[rtsidx+1] - 5) and (i + RNN_LEN) < len(DAC):\n",
    "            # check if we should include another label\n",
    "            while RTS[l] <= i + RNN_LEN and l < len(REF):\n",
    "                labels.append(REF[l])\n",
    "                l += 1\n",
    "            data.append([\n",
    "                    DAC[i:(i+RNN_LEN)],\n",
    "                    labels\n",
    "            ])\n",
    "            i += 5\n",
    "    return data\n",
    "\n",
    "# pp = partial(processRead, filename=filename)\n",
    "# pp(readIDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = Pool(16)\n",
    "results_prim = pool.map(partial(processRead, filename=filename), readIDs[:16])\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for res in results_prim:\n",
    "    results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_dacs(dac):\n",
    "    dmin = min(dac)\n",
    "    dmax = max(dac)\n",
    "    return [[(d-dmin)/(dmax-dmin)] for d in dac]\n",
    "\n",
    "# to test without CTC\n",
    "def ohe(v):\n",
    "    tr = np.array([0,0,0,0])\n",
    "    tr[v] = 1\n",
    "    return tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([normalise_dacs(r[0]) for r in results])\n",
    "y = np.array([r[1] for r in results])\n",
    "simple_y = np.array([ohe(yy[-1]) for yy in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0][0])\n",
    "print(y[0])\n",
    "print(simple_y[0])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(32,return_sequences=True))\n",
    "model.add(Conv1D(32, 3,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\", \n",
    "          input_shape=X[0].shape))\n",
    "model.add(Conv1D(32, 10,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=X, y=simple_y, batch_size=10, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(X[:10])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
